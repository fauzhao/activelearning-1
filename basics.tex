% !TEX root = ./active_learning.tex

We assume a prior $\prior{w}$ where $\vek w$ is a vector of parameters that describe our model.
As the model we use a sigmoid function
\begin{align}
	\sigmoid{w_0, w_1, x} = \frac{1}{1 + \exp[-(w_0 - w_1 x)]}
\end{align}
\note{This might not be the best way to write the sigmoid.}
We collect data $n$ data points by presenting a stimulus $x \in \mathbb{R}$ and observing a binary response $y \in \set{0, 1}$:
\begin{align}
	 D = \set{(x_1, y_1), \dots, (x_n, y_n)} \equiv (X, Y)
	 % D = \set{y_1, \dots, y_n} \equiv (X, Y)
\end{align}
The likelihood of the parameters $\vek w$ given the data $D$ is given by
\begin{align}
	\prob{Y \mid \vek w, X}
	&= \prod_{i=1}^n \prob{y_i \mid \vek w, x_i} \\
	&= \prod_{i=1}^n \sigmoid{w_0 + w_1 x}^{y_i}
		\left( 1 - \sigmoid{w_0 + w_1 x} \right)^{1-y_i}
\end{align}
\note{The stimuli $x$ are not considered part of the data, because we have control over it}
The posterior probability of the parameters $\vek w$ is
\begin{align}
	\prob{\vek w \mid X, Y} = \frac{\prob{Y \mid w, X} \prior{\vek w}}{\prob{Y \mid X}}
\end{align}
The denominator, \ie the marginal of the likelihood, is computed by taking the integral over all hypotheses:
\begin{align}
	\int \prob{Y \mid \vek w', X} \prior{\vek w'} \ud \vek w'
	&= \iint \prob{Y \mid w_0, w_1, X} \prior{w_0, w_1} \ud w_0' \ud w_1'
\end{align}
The goal is to get a posterior $\prob{\vek \mid X, Y}$ that is of low uncertainty. We use entropy as a measure of the current uncertainty of our estimation of $\vek w$. By \emph{current} we mean that we use the data we have discovered in the $n$ steps until now. To make this clear we write $X_n, Y_n$ instead of $X, Y$:
\begin{align}
	H(\vek w \mid X_n Y_n) = - \int \prob{\vek w' \mid X_n, Y_n} \log[\prob{\vek w' \mid X_n, Y_n}] \ud \vek w' \,.
\end{align}
In principle we would now like to choose our next stimulus $x_{n+1}$ such that it minimizes the resulting entropy $H(\vek w \mid X_n, Y_n, x_{n+1}, y_{n+1})$, but we do not know what $y_{n+1}$ is. So we want to find the $x_{n+1}$ that minimizes the mean:
\begin{align}
	K(x_{n+1}) =&~ H(\vek w \mid X_n, Y_n, x_{n+1}, y_{n+1} = 0) ~
	\prob{y_{n+1} = 0 \mid X_n, Y_n, x_{n+1}} \\
	&+
	H(\vek w \mid X_n, Y_n, x_{n+1}, y_{n+1} = 1) ~
	\prob{y_{n+1} = 1 \mid X_n, Y_n, x_{n+1}} \,.
\end{align}
Here $\prob{y_{n+1} = 0/1 \mid X_n, Y_n, x_{n+1}}$ is called the \emph{predictive distribution}. Determining them again requires an integral over the hypotheses:
\begin{align}
	\prob{y_{n+1} = 0/1 \mid X_n, Y_n, x_{n+1}}
	= \int \prob{y_{n+1} = 0/1 \mid \vek w', x_{n+1}} \prob{\vek w' \mid X_n, Y_n, x_{n+1}, y_{n+1} = 0/1} \ud \vek w'
\end{align}
